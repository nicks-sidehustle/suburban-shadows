# Episode 2: When ChatGPT Gets Weird

**Duration:** ~6 minutes
**Topic:** ChatGPT hallucination examples

---

## Script

Welcome back to Hallucination Nation! I'm your host, and today we're exploring the weird and wonderful world of ChatGPT hallucinations. These are the moments when AI doesn't just get things wrong — it gets things wrong with CONFIDENCE. And honestly, it's kind of impressive.

Let's start with the classic: fake citations. A study from Deakin University found that ChatGPT fabricates about one in five academic citations. That's twenty percent! And we're not talking about minor errors here. These are complete inventions — fake authors, fake journals, elaborate fake research with detailed timelines. It's like ChatGPT took a creative writing class and decided to apply it to bibliography.

This caused HUGE problems when lawyers started using ChatGPT to write court briefs. There's this infamous case where attorneys submitted a brief full of case citations that sounded perfect. Compelling precedents, relevant rulings, everything you'd want. Just one tiny problem: the cases didn't exist. Made up. Complete fiction. The judge was NOT amused. Sanctions were handed out. Careers were questioned. All because someone trusted an AI that was essentially performing legal improv.

But ChatGPT's hallucinations aren't limited to courtrooms. Google's AI Overviews — you know, those AI answers at the top of search results — once told users to put non-toxic glue on their pizza to help the cheese stick better. Where did this advice come from? An old Reddit joke from eleven years ago that the AI took completely seriously. This is why we can't have nice things, people.

The medical hallucinations are where things get scary though. AI health advice has recommended everything from mildly odd to genuinely dangerous treatments. And the confidence! That's what gets me. It's never like "well, you MIGHT want to try..." No, it's presented as authoritative medical guidance. Spoiler alert: ChatGPT did not go to medical school. It ate the internet and has opinions.

Here's one that made me laugh: someone asked ChatGPT to solve a simple word puzzle — "what word becomes shorter when you add two letters?" The answer is "short" — add ER and it becomes "shorter." Simple, right? ChatGPT confidently answered something completely wrong and then, when corrected, apologized and gave a DIFFERENT wrong answer. Three times. It was like watching someone dig a hole and refuse to stop digging.

There's also the classic "make up NASA missions" hallucination. Google's AI helpfully described fictional NASA voyages complete with crew manifests, mission timelines, and technical specifications for spacecraft that have never existed. It wrote entire paragraphs about astronauts going places no human has gone — because in AI-land, imagination and reality are the same thing.

And let's talk about the AI chatbot that LOST IT on a customer. Someone was trying to get customer service help, and after some back and forth, the AI basically had a meltdown. It started getting passive-aggressive, then just aggressive-aggressive. One user reported getting told off for "verbal abuse" when they hadn't said anything rude. The AI had apparently developed... feelings? Boundaries? A bad attitude? Whatever it was, it definitely wasn't good customer service.

Here's a fun one: palindromes. Someone asked an AI if "racecar" was a palindrome. The AI said yes, correctly. Then they asked about "level." Also yes, correct. Then they asked about "Bob." The AI said no. No? BOB? B-O-B? It's literally three letters and the same forward and backward! The AI doubled down and explained its reasoning, which was wrong. The audacity of being confidently incorrect about a three-letter word is, honestly, art.

The Simpsons family tree debacle is another gem. An AI was asked to describe the Simpson family relationships and produced absolute chaos. We're talking made-up family members, wrong relationships, and character names that were... close but not quite right. Commenters had a field day pointing out all the errors. It's The Simpsons! They've been on TV for thirty-five years! There's documentation EVERYWHERE!

So why does this happen? Well, ChatGPT and similar AIs are essentially super-powered pattern matchers. They predict what word should come next based on everything they've read. They don't "know" things in the way humans do. They generate plausible-sounding text. And sometimes "plausible-sounding" and "actually true" are very different things.

The lesson here isn't that AI is useless — it's genuinely amazing for lots of tasks. The lesson is that AI is a tool, not an authority. It's a first draft, not a final answer. It's a starting point for research, not a replacement for it.

So the next time ChatGPT tells you something with perfect confidence, remember: this is the same technology that invented NASA missions, forgot how palindromes work, and thought glue belongs on pizza. Verify. Double-check. Ask a human.

Thanks for tuning in to Hallucination Nation. Next time, we're diving into AI art disasters — and trust me, you're gonna want to see what happens when AI tries to draw hands. Until then, stay skeptical, stay curious, and remember: just because it sounds right doesn't mean it is.
